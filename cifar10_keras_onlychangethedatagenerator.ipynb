{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NZtechpapa/Cifar10SVM_CNN/blob/master/cifar10_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jmTIeY4EMtBk",
    "outputId": "776d7054-89f9-413c-ff6d-2e720926b809"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "e0z5OV0qMuF2",
    "outputId": "8cb89b72-60ad-4f5a-a9b3-a1f16aeb6a82"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    lrate = 0.001\n",
    "    if epoch > 75:\n",
    "        lrate = 0.0005\n",
    "    elif epoch > 100:\n",
    "        lrate = 0.0003        \n",
    "    return lrate\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cifar10 train -  rows: 50000  columns: 32\n",
      "Cifar10 test -  rows: 10000  columns: 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cifar10 train -  rows:\",x_train.shape[0],\" columns:\", x_train.shape[1])\n",
    "print(\"Cifar10 test -  rows:\",x_test.shape[0],\" columns:\", x_test.shape[1])\n",
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'automobile'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict = {0:'ariplane',1:'automobile',2:'bird',3:'cat',4:'deer',5:'dog',6:'frog',7:'horse',8:'ship',9:'truck'}\n",
    "label_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lUEbz2ZoMyrd"
   },
   "outputs": [],
   "source": [
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_X1u_tF2M1Nv"
   },
   "outputs": [],
   "source": [
    "\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1020
    },
    "colab_type": "code",
    "id": "UGt-pp-9M7Rr",
    "outputId": "481b088a-ced7-4bfb-ea73-43e475d526e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dp1/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/dp1/.conda/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "svm (Dense)                  (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 1e-4\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(num_classes, activation='softmax'))\n",
    "model.add(Dense(num_classes, activation='linear',name='svm'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLSfCxsbM-uh"
   },
   "outputs": [],
   "source": [
    "\n",
    "#data augmentation\n",
    "#datagen = ImageDataGenerator(\n",
    "#    rotation_range=15,\n",
    "#    width_shift_range=0.1,\n",
    "#    height_shift_range=0.1,\n",
    "#    horizontal_flip=True,\n",
    "#    )\n",
    "datagen = ImageDataGenerator(\n",
    "    # set input mean to 0 over the dataset\n",
    "    featurewise_center=False,\n",
    "    # set each sample mean to 0\n",
    "    samplewise_center=False,\n",
    "    # divide inputs by std of dataset\n",
    "    featurewise_std_normalization=False,\n",
    "    # divide each input by its std\n",
    "    samplewise_std_normalization=False,\n",
    "    # apply ZCA whitening\n",
    "    zca_whitening=False,\n",
    "    # epsilon for ZCA whitening\n",
    "    zca_epsilon=1e-06,\n",
    "    # randomly rotate images in the range (deg 0 to 180)\n",
    "    rotation_range=0,\n",
    "    # randomly shift images horizontally\n",
    "    width_shift_range=0.1,\n",
    "    # randomly shift images vertically\n",
    "    height_shift_range=0.1,\n",
    "    # set range for random shear\n",
    "    shear_range=0.,\n",
    "    # set range for random zoom\n",
    "    zoom_range=0.,\n",
    "    # set range for random channel shifts\n",
    "    channel_shift_range=0.,\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    # value used for fill_mode = \"constant\"\n",
    "    cval=0.,\n",
    "    # randomly flip images\n",
    "    horizontal_flip=True,\n",
    "    # randomly flip images\n",
    "    vertical_flip=False,\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0)\n",
    "\n",
    "\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OVDwVt_XNUY5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtIhgZFDNnVo"
   },
   "outputs": [],
   "source": [
    "metrics = ['accuracy']\n",
    "#optimizer = keras.optimizers.RMSprop(lr=2e-3, decay=1e-5)\n",
    "#optimizer = tf.train.AdamOptimizer(1.e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zjUMZyCXNFVZ"
   },
   "outputs": [],
   "source": [
    "#training\n",
    "batch_size = 64\n",
    "\n",
    "#opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "optimizer = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "jILrkDSJNIBw",
    "outputId": "51afed39-cb68-4a48-c0ef-65ee7e9bad0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dp1/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/dp1/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/125\n",
      "781/781 [==============================] - 211s 270ms/step - loss: 0.8841 - acc: 0.3770 - val_loss: 0.5094 - val_acc: 0.4752\n",
      "Epoch 2/125\n",
      "781/781 [==============================] - 180s 230ms/step - loss: 0.4477 - acc: 0.5370 - val_loss: 0.4245 - val_acc: 0.5853\n",
      "Epoch 3/125\n",
      "781/781 [==============================] - 170s 217ms/step - loss: 0.3963 - acc: 0.6059 - val_loss: 0.3743 - val_acc: 0.6336\n",
      "Epoch 4/125\n",
      "781/781 [==============================] - 173s 221ms/step - loss: 0.3673 - acc: 0.6494 - val_loss: 0.3318 - val_acc: 0.6896\n",
      "Epoch 5/125\n",
      "781/781 [==============================] - 171s 218ms/step - loss: 0.3475 - acc: 0.6757 - val_loss: 0.3338 - val_acc: 0.6891\n",
      "Epoch 6/125\n",
      "781/781 [==============================] - 166s 212ms/step - loss: 0.3345 - acc: 0.6963 - val_loss: 0.3063 - val_acc: 0.7278\n",
      "Epoch 7/125\n",
      "781/781 [==============================] - 171s 219ms/step - loss: 0.3270 - acc: 0.7085 - val_loss: 0.3118 - val_acc: 0.7233\n",
      "Epoch 8/125\n",
      "781/781 [==============================] - 201s 258ms/step - loss: 0.3214 - acc: 0.7168 - val_loss: 0.3224 - val_acc: 0.7179\n",
      "Epoch 9/125\n",
      "781/781 [==============================] - 218s 279ms/step - loss: 0.3141 - acc: 0.7261 - val_loss: 0.2916 - val_acc: 0.7585\n",
      "Epoch 10/125\n",
      "781/781 [==============================] - 203s 259ms/step - loss: 0.3135 - acc: 0.7300 - val_loss: 0.3351 - val_acc: 0.7018\n",
      "Epoch 11/125\n",
      "781/781 [==============================] - 183s 234ms/step - loss: 0.3096 - acc: 0.7368 - val_loss: 0.2928 - val_acc: 0.7444\n",
      "Epoch 12/125\n",
      "781/781 [==============================] - 169s 216ms/step - loss: 0.3061 - acc: 0.7410 - val_loss: 0.2751 - val_acc: 0.7738\n",
      "Epoch 13/125\n",
      "781/781 [==============================] - 171s 219ms/step - loss: 0.3049 - acc: 0.7450 - val_loss: 0.3092 - val_acc: 0.7321\n",
      "Epoch 14/125\n",
      "781/781 [==============================] - 171s 219ms/step - loss: 0.3007 - acc: 0.7494 - val_loss: 0.2800 - val_acc: 0.7731\n",
      "Epoch 15/125\n",
      "781/781 [==============================] - 170s 217ms/step - loss: 0.3012 - acc: 0.7512 - val_loss: 0.2901 - val_acc: 0.7595\n",
      "Epoch 16/125\n",
      "781/781 [==============================] - 167s 214ms/step - loss: 0.3007 - acc: 0.7551 - val_loss: 0.3159 - val_acc: 0.7501\n",
      "Epoch 17/125\n",
      "781/781 [==============================] - 172s 220ms/step - loss: 0.2975 - acc: 0.7576 - val_loss: 0.2786 - val_acc: 0.7762\n",
      "Epoch 18/125\n",
      "781/781 [==============================] - 202s 258ms/step - loss: 0.2952 - acc: 0.7592 - val_loss: 0.2804 - val_acc: 0.7759\n",
      "Epoch 19/125\n",
      "781/781 [==============================] - 219s 281ms/step - loss: 0.2969 - acc: 0.7580 - val_loss: 0.3115 - val_acc: 0.7380\n",
      "Epoch 20/125\n",
      "781/781 [==============================] - 215s 275ms/step - loss: 0.2923 - acc: 0.7671 - val_loss: 0.2616 - val_acc: 0.7948\n",
      "Epoch 21/125\n",
      "781/781 [==============================] - 222s 284ms/step - loss: 0.2920 - acc: 0.7610 - val_loss: 0.2696 - val_acc: 0.7897\n",
      "Epoch 22/125\n",
      "781/781 [==============================] - 215s 275ms/step - loss: 0.2914 - acc: 0.7659 - val_loss: 0.2901 - val_acc: 0.7600\n",
      "Epoch 23/125\n",
      "781/781 [==============================] - 191s 245ms/step - loss: 0.2909 - acc: 0.7685 - val_loss: 0.2965 - val_acc: 0.7676\n",
      "Epoch 24/125\n",
      "781/781 [==============================] - 216s 276ms/step - loss: 0.2885 - acc: 0.7699 - val_loss: 0.2759 - val_acc: 0.7869\n",
      "Epoch 25/125\n",
      "781/781 [==============================] - 170s 218ms/step - loss: 0.2888 - acc: 0.7722 - val_loss: 0.2604 - val_acc: 0.8001\n",
      "Epoch 26/125\n",
      "781/781 [==============================] - 173s 221ms/step - loss: 0.2873 - acc: 0.7721 - val_loss: 0.2761 - val_acc: 0.7890\n",
      "Epoch 27/125\n",
      "781/781 [==============================] - 201s 257ms/step - loss: 0.2877 - acc: 0.7715 - val_loss: 0.2625 - val_acc: 0.7992\n",
      "Epoch 28/125\n",
      "781/781 [==============================] - 201s 257ms/step - loss: 0.2878 - acc: 0.7720 - val_loss: 0.2778 - val_acc: 0.7865\n",
      "Epoch 29/125\n",
      "781/781 [==============================] - 224s 286ms/step - loss: 0.2855 - acc: 0.7759 - val_loss: 0.2832 - val_acc: 0.7830\n",
      "Epoch 30/125\n",
      "781/781 [==============================] - 217s 278ms/step - loss: 0.2824 - acc: 0.7785 - val_loss: 0.2824 - val_acc: 0.7743\n",
      "Epoch 31/125\n",
      "781/781 [==============================] - 196s 251ms/step - loss: 0.2835 - acc: 0.7784 - val_loss: 0.2667 - val_acc: 0.7947\n",
      "Epoch 32/125\n",
      "781/781 [==============================] - 174s 223ms/step - loss: 0.2844 - acc: 0.7782 - val_loss: 0.2812 - val_acc: 0.7808\n",
      "Epoch 33/125\n",
      "781/781 [==============================] - 175s 224ms/step - loss: 0.2835 - acc: 0.7792 - val_loss: 0.2602 - val_acc: 0.8092\n",
      "Epoch 34/125\n",
      "781/781 [==============================] - 176s 225ms/step - loss: 0.2822 - acc: 0.7799 - val_loss: 0.2790 - val_acc: 0.7818\n",
      "Epoch 35/125\n",
      "781/781 [==============================] - 177s 226ms/step - loss: 0.2799 - acc: 0.7829 - val_loss: 0.2684 - val_acc: 0.7945\n",
      "Epoch 36/125\n",
      "781/781 [==============================] - 178s 227ms/step - loss: 0.2800 - acc: 0.7837 - val_loss: 0.2701 - val_acc: 0.7920\n",
      "Epoch 37/125\n",
      "781/781 [==============================] - 174s 222ms/step - loss: 0.2782 - acc: 0.7847 - val_loss: 0.2742 - val_acc: 0.7908\n",
      "Epoch 38/125\n",
      "781/781 [==============================] - 175s 225ms/step - loss: 0.2793 - acc: 0.7858 - val_loss: 0.2625 - val_acc: 0.8031\n",
      "Epoch 39/125\n",
      "781/781 [==============================] - 173s 221ms/step - loss: 0.2787 - acc: 0.7860 - val_loss: 0.2538 - val_acc: 0.8172\n",
      "Epoch 40/125\n",
      "781/781 [==============================] - 172s 221ms/step - loss: 0.2779 - acc: 0.7844 - val_loss: 0.3031 - val_acc: 0.7710\n",
      "Epoch 41/125\n",
      "781/781 [==============================] - 177s 226ms/step - loss: 0.2771 - acc: 0.7883 - val_loss: 0.2594 - val_acc: 0.8099\n",
      "Epoch 42/125\n",
      "781/781 [==============================] - 177s 226ms/step - loss: 0.2804 - acc: 0.7860 - val_loss: 0.2486 - val_acc: 0.8260\n",
      "Epoch 43/125\n",
      "781/781 [==============================] - 191s 245ms/step - loss: 0.2754 - acc: 0.7899 - val_loss: 0.3005 - val_acc: 0.7662\n",
      "Epoch 44/125\n",
      "781/781 [==============================] - 204s 261ms/step - loss: 0.2772 - acc: 0.7891 - val_loss: 0.2643 - val_acc: 0.7991\n",
      "Epoch 45/125\n",
      "781/781 [==============================] - 216s 277ms/step - loss: 0.2757 - acc: 0.7895 - val_loss: 0.2540 - val_acc: 0.8136\n",
      "Epoch 46/125\n",
      "781/781 [==============================] - 174s 223ms/step - loss: 0.2761 - acc: 0.7887 - val_loss: 0.2687 - val_acc: 0.7944\n",
      "Epoch 47/125\n",
      "781/781 [==============================] - 202s 258ms/step - loss: 0.2744 - acc: 0.7901 - val_loss: 0.2697 - val_acc: 0.7936\n",
      "Epoch 48/125\n",
      "781/781 [==============================] - 192s 246ms/step - loss: 0.2752 - acc: 0.7906 - val_loss: 0.2624 - val_acc: 0.7978\n",
      "Epoch 49/125\n",
      "781/781 [==============================] - 182s 233ms/step - loss: 0.2751 - acc: 0.7914 - val_loss: 0.2670 - val_acc: 0.8007\n",
      "Epoch 50/125\n",
      "781/781 [==============================] - 175s 224ms/step - loss: 0.2745 - acc: 0.7918 - val_loss: 0.2565 - val_acc: 0.8117\n",
      "Epoch 51/125\n",
      "781/781 [==============================] - 177s 226ms/step - loss: 0.2730 - acc: 0.7929 - val_loss: 0.2470 - val_acc: 0.8222\n",
      "Epoch 52/125\n",
      "781/781 [==============================] - 219s 280ms/step - loss: 0.2734 - acc: 0.7947 - val_loss: 0.2501 - val_acc: 0.8201\n",
      "Epoch 53/125\n",
      "781/781 [==============================] - 205s 263ms/step - loss: 0.2724 - acc: 0.7921 - val_loss: 0.2554 - val_acc: 0.8152\n",
      "Epoch 54/125\n",
      "781/781 [==============================] - 177s 227ms/step - loss: 0.2699 - acc: 0.7950 - val_loss: 0.2671 - val_acc: 0.7926\n",
      "Epoch 55/125\n",
      "781/781 [==============================] - 201s 258ms/step - loss: 0.2726 - acc: 0.7917 - val_loss: 0.2704 - val_acc: 0.7957\n",
      "Epoch 56/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 201s 258ms/step - loss: 0.2724 - acc: 0.7942 - val_loss: 0.2616 - val_acc: 0.7994\n",
      "Epoch 57/125\n",
      "781/781 [==============================] - 193s 247ms/step - loss: 0.2697 - acc: 0.7971 - val_loss: 0.2527 - val_acc: 0.8120\n",
      "Epoch 58/125\n",
      "781/781 [==============================] - 173s 222ms/step - loss: 0.2710 - acc: 0.7964 - val_loss: 0.2561 - val_acc: 0.8169\n",
      "Epoch 59/125\n",
      "781/781 [==============================] - 216s 277ms/step - loss: 0.2692 - acc: 0.7955 - val_loss: 0.2659 - val_acc: 0.7944\n",
      "Epoch 60/125\n",
      "781/781 [==============================] - 209s 267ms/step - loss: 0.2710 - acc: 0.7965 - val_loss: 0.3135 - val_acc: 0.7575\n",
      "Epoch 61/125\n",
      "781/781 [==============================] - 201s 257ms/step - loss: 0.2685 - acc: 0.7972 - val_loss: 0.2680 - val_acc: 0.7951\n",
      "Epoch 62/125\n",
      "781/781 [==============================] - 218s 279ms/step - loss: 0.2683 - acc: 0.7976 - val_loss: 0.2981 - val_acc: 0.7736\n",
      "Epoch 63/125\n",
      "781/781 [==============================] - 212s 271ms/step - loss: 0.2681 - acc: 0.7966 - val_loss: 0.2488 - val_acc: 0.8234\n",
      "Epoch 64/125\n",
      "781/781 [==============================] - 204s 261ms/step - loss: 0.2691 - acc: 0.7958 - val_loss: 0.2875 - val_acc: 0.7797\n",
      "Epoch 65/125\n",
      "781/781 [==============================] - 185s 236ms/step - loss: 0.2672 - acc: 0.7983 - val_loss: 0.2756 - val_acc: 0.7773\n",
      "Epoch 66/125\n",
      "781/781 [==============================] - 197s 253ms/step - loss: 0.2672 - acc: 0.7971 - val_loss: 0.2440 - val_acc: 0.8211\n",
      "Epoch 67/125\n",
      "781/781 [==============================] - 210s 269ms/step - loss: 0.2671 - acc: 0.7984 - val_loss: 0.2499 - val_acc: 0.8145\n",
      "Epoch 68/125\n",
      "781/781 [==============================] - 188s 241ms/step - loss: 0.2686 - acc: 0.7974 - val_loss: 0.3068 - val_acc: 0.7700\n",
      "Epoch 69/125\n",
      "781/781 [==============================] - 176s 225ms/step - loss: 0.2679 - acc: 0.7972 - val_loss: 0.2375 - val_acc: 0.8282\n",
      "Epoch 70/125\n",
      "781/781 [==============================] - 173s 222ms/step - loss: 0.2650 - acc: 0.7993 - val_loss: 0.2561 - val_acc: 0.8127\n",
      "Epoch 71/125\n",
      "781/781 [==============================] - 172s 221ms/step - loss: 0.2653 - acc: 0.8004 - val_loss: 0.2425 - val_acc: 0.8229\n",
      "Epoch 72/125\n",
      "781/781 [==============================] - 170s 218ms/step - loss: 0.2664 - acc: 0.7980 - val_loss: 0.2449 - val_acc: 0.8181\n",
      "Epoch 73/125\n",
      "781/781 [==============================] - 176s 225ms/step - loss: 0.2652 - acc: 0.7993 - val_loss: 0.2355 - val_acc: 0.8352\n",
      "Epoch 74/125\n",
      "781/781 [==============================] - 173s 222ms/step - loss: 0.2666 - acc: 0.7975 - val_loss: 0.2662 - val_acc: 0.7922\n",
      "Epoch 75/125\n",
      "781/781 [==============================] - 176s 225ms/step - loss: 0.2666 - acc: 0.8003 - val_loss: 0.2648 - val_acc: 0.7987\n",
      "Epoch 76/125\n",
      "781/781 [==============================] - 177s 226ms/step - loss: 0.2639 - acc: 0.7981 - val_loss: 0.2735 - val_acc: 0.7880\n",
      "Epoch 77/125\n",
      "781/781 [==============================] - 212s 271ms/step - loss: 0.2288 - acc: 0.8269 - val_loss: 0.2104 - val_acc: 0.8430\n",
      "Epoch 78/125\n",
      "781/781 [==============================] - 217s 278ms/step - loss: 0.2221 - acc: 0.8323 - val_loss: 0.2247 - val_acc: 0.8299\n",
      "Epoch 79/125\n",
      "781/781 [==============================] - 217s 278ms/step - loss: 0.2176 - acc: 0.8319 - val_loss: 0.1959 - val_acc: 0.8539\n",
      "Epoch 80/125\n",
      "781/781 [==============================] - 180s 231ms/step - loss: 0.2159 - acc: 0.8346 - val_loss: 0.1994 - val_acc: 0.8512\n",
      "Epoch 81/125\n",
      "781/781 [==============================] - 175s 224ms/step - loss: 0.2151 - acc: 0.8348 - val_loss: 0.1937 - val_acc: 0.8600\n",
      "Epoch 82/125\n",
      "781/781 [==============================] - 182s 233ms/step - loss: 0.2127 - acc: 0.8367 - val_loss: 0.2073 - val_acc: 0.8355\n",
      "Epoch 83/125\n",
      "781/781 [==============================] - 202s 259ms/step - loss: 0.2120 - acc: 0.8367 - val_loss: 0.2048 - val_acc: 0.8399\n",
      "Epoch 84/125\n",
      "781/781 [==============================] - 172s 220ms/step - loss: 0.2098 - acc: 0.8368 - val_loss: 0.2086 - val_acc: 0.8411\n",
      "Epoch 85/125\n",
      "781/781 [==============================] - 172s 220ms/step - loss: 0.2088 - acc: 0.8380 - val_loss: 0.2175 - val_acc: 0.8286\n",
      "Epoch 86/125\n",
      "781/781 [==============================] - 191s 245ms/step - loss: 0.2077 - acc: 0.8385 - val_loss: 0.2003 - val_acc: 0.8408\n",
      "Epoch 87/125\n",
      "781/781 [==============================] - 215s 276ms/step - loss: 0.2088 - acc: 0.8373 - val_loss: 0.2002 - val_acc: 0.8408\n",
      "Epoch 88/125\n",
      "781/781 [==============================] - 191s 245ms/step - loss: 0.2064 - acc: 0.8404 - val_loss: 0.2057 - val_acc: 0.8343\n",
      "Epoch 89/125\n",
      "781/781 [==============================] - 178s 227ms/step - loss: 0.2077 - acc: 0.8365 - val_loss: 0.1979 - val_acc: 0.8445\n",
      "Epoch 90/125\n",
      "781/781 [==============================] - 214s 274ms/step - loss: 0.2035 - acc: 0.8405 - val_loss: 0.1886 - val_acc: 0.8560\n",
      "Epoch 91/125\n",
      "781/781 [==============================] - 213s 273ms/step - loss: 0.2053 - acc: 0.8407 - val_loss: 0.2082 - val_acc: 0.8363\n",
      "Epoch 92/125\n",
      "781/781 [==============================] - 210s 269ms/step - loss: 0.2053 - acc: 0.8382 - val_loss: 0.2042 - val_acc: 0.8427\n",
      "Epoch 93/125\n",
      "781/781 [==============================] - 214s 273ms/step - loss: 0.2031 - acc: 0.8420 - val_loss: 0.1884 - val_acc: 0.8600\n",
      "Epoch 94/125\n",
      "781/781 [==============================] - 214s 273ms/step - loss: 0.2063 - acc: 0.8389 - val_loss: 0.1860 - val_acc: 0.8606\n",
      "Epoch 95/125\n",
      "781/781 [==============================] - 179s 229ms/step - loss: 0.2029 - acc: 0.8421 - val_loss: 0.1888 - val_acc: 0.8577\n",
      "Epoch 96/125\n",
      "781/781 [==============================] - 213s 273ms/step - loss: 0.2042 - acc: 0.8404 - val_loss: 0.2051 - val_acc: 0.8392\n",
      "Epoch 97/125\n",
      "781/781 [==============================] - 178s 228ms/step - loss: 0.2031 - acc: 0.8420 - val_loss: 0.1983 - val_acc: 0.8432\n",
      "Epoch 98/125\n",
      "781/781 [==============================] - 178s 228ms/step - loss: 0.2030 - acc: 0.8408 - val_loss: 0.1952 - val_acc: 0.8522\n",
      "Epoch 99/125\n",
      "781/781 [==============================] - 214s 274ms/step - loss: 0.2027 - acc: 0.8401 - val_loss: 0.1832 - val_acc: 0.8626\n",
      "Epoch 100/125\n",
      "781/781 [==============================] - 212s 271ms/step - loss: 0.2019 - acc: 0.8401 - val_loss: 0.1942 - val_acc: 0.8525\n",
      "Epoch 101/125\n",
      "781/781 [==============================] - 190s 244ms/step - loss: 0.2009 - acc: 0.8418 - val_loss: 0.1916 - val_acc: 0.8548\n",
      "Epoch 102/125\n",
      "781/781 [==============================] - 208s 267ms/step - loss: 0.2029 - acc: 0.8399 - val_loss: 0.2021 - val_acc: 0.8462\n",
      "Epoch 103/125\n",
      "781/781 [==============================] - 180s 231ms/step - loss: 0.1999 - acc: 0.8437 - val_loss: 0.2106 - val_acc: 0.8310\n",
      "Epoch 104/125\n",
      "781/781 [==============================] - 172s 220ms/step - loss: 0.2022 - acc: 0.8409 - val_loss: 0.1998 - val_acc: 0.8418\n",
      "Epoch 105/125\n",
      "781/781 [==============================] - 176s 226ms/step - loss: 0.2017 - acc: 0.8423 - val_loss: 0.1963 - val_acc: 0.8465\n",
      "Epoch 106/125\n",
      "781/781 [==============================] - 176s 225ms/step - loss: 0.2021 - acc: 0.8413 - val_loss: 0.1991 - val_acc: 0.8495\n",
      "Epoch 107/125\n",
      "781/781 [==============================] - 174s 223ms/step - loss: 0.2007 - acc: 0.8431 - val_loss: 0.1960 - val_acc: 0.8541\n",
      "Epoch 108/125\n",
      "781/781 [==============================] - 178s 228ms/step - loss: 0.2006 - acc: 0.8444 - val_loss: 0.1846 - val_acc: 0.8586\n",
      "Epoch 109/125\n",
      "781/781 [==============================] - 199s 255ms/step - loss: 0.2026 - acc: 0.8387 - val_loss: 0.1951 - val_acc: 0.8525\n",
      "Epoch 110/125\n",
      "781/781 [==============================] - 193s 247ms/step - loss: 0.2005 - acc: 0.8440 - val_loss: 0.1974 - val_acc: 0.8468\n",
      "Epoch 111/125\n",
      "781/781 [==============================] - 174s 223ms/step - loss: 0.2000 - acc: 0.8446 - val_loss: 0.1889 - val_acc: 0.8593\n",
      "Epoch 112/125\n",
      "781/781 [==============================] - 175s 225ms/step - loss: 0.2000 - acc: 0.8441 - val_loss: 0.1879 - val_acc: 0.8602\n",
      "Epoch 113/125\n",
      "781/781 [==============================] - 184s 236ms/step - loss: 0.2013 - acc: 0.8433 - val_loss: 0.1937 - val_acc: 0.8478\n",
      "Epoch 114/125\n",
      "781/781 [==============================] - 183s 234ms/step - loss: 0.1989 - acc: 0.8452 - val_loss: 0.2006 - val_acc: 0.8449\n",
      "Epoch 115/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 191s 245ms/step - loss: 0.1998 - acc: 0.8445 - val_loss: 0.1963 - val_acc: 0.8544\n",
      "Epoch 116/125\n",
      "781/781 [==============================] - 214s 274ms/step - loss: 0.1983 - acc: 0.8451 - val_loss: 0.2020 - val_acc: 0.8391\n",
      "Epoch 117/125\n",
      "781/781 [==============================] - 173s 221ms/step - loss: 0.2011 - acc: 0.8423 - val_loss: 0.2020 - val_acc: 0.8414\n",
      "Epoch 118/125\n",
      "781/781 [==============================] - 168s 215ms/step - loss: 0.1991 - acc: 0.8441 - val_loss: 0.1927 - val_acc: 0.8512\n",
      "Epoch 119/125\n",
      "781/781 [==============================] - 172s 220ms/step - loss: 0.2006 - acc: 0.8430 - val_loss: 0.1856 - val_acc: 0.8633\n",
      "Epoch 120/125\n",
      "781/781 [==============================] - 174s 223ms/step - loss: 0.2001 - acc: 0.8431 - val_loss: 0.2048 - val_acc: 0.8415\n",
      "Epoch 121/125\n",
      "781/781 [==============================] - 175s 224ms/step - loss: 0.1990 - acc: 0.8434 - val_loss: 0.1941 - val_acc: 0.8487\n",
      "Epoch 122/125\n",
      "781/781 [==============================] - 174s 223ms/step - loss: 0.1985 - acc: 0.8470 - val_loss: 0.1971 - val_acc: 0.8514\n",
      "Epoch 123/125\n",
      "781/781 [==============================] - 172s 220ms/step - loss: 0.1987 - acc: 0.8460 - val_loss: 0.1948 - val_acc: 0.8489\n",
      "Epoch 124/125\n",
      "781/781 [==============================] - 203s 259ms/step - loss: 0.1989 - acc: 0.8434 - val_loss: 0.1956 - val_acc: 0.8440\n",
      "Epoch 125/125\n",
      "781/781 [==============================] - 187s 239ms/step - loss: 0.1979 - acc: 0.8463 - val_loss: 0.1880 - val_acc: 0.8584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ad8864d30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=x_train.shape[0] //( batch_size),epochs=125,\\\n",
    "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1278
    },
    "colab_type": "code",
    "id": "hlhJaP_JGOev",
    "outputId": "1c82d37e-03a3-4e7b-da28-d72eec9791c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 13s 1ms/step\n",
      "\n",
      "Test result: 85.840 loss: 0.188\n"
     ]
    }
   ],
   "source": [
    "#save to disk\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')    \n",
    "\n",
    "#testing\n",
    "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "#from utils import log_progress\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (124,) and (125,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-21434deecd10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepoch_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m124\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m125\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 231\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (124,) and (125,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsoAAAEVCAYAAAD5FsMnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHbVJREFUeJzt3Xu4ZFV95vHvK4g3EFTaC9CKCqgdEm8t4ngBg3EAI2hiFBxH9EGJSQiTiUaJZhAxmniJjhgcReMlRq6amI5BcWIEE0MrrSIRGLQFtFsUWqABRQT0N3/sfejq6nX6VB+qzulz+vt5nvN07apVu9batevXb629qypVhSRJkqSN3W2+OyBJkiRtjQzKkiRJUoNBWZIkSWowKEuSJEkNBmVJkiSpwaAsSZIkNRiUpXmQ5LNJjprvfsyHdD6S5IYkX53v/mxNkrw+yYc2c/vLkvz7XPZp4LGfnuTy+XhszbxvSJoMg7K0GUmuSvKzJD/pg90/J1l6V9dbVYdU1cdm0Z8kOS7Jt5L8NMnaJGcn+dX+9o8mqST7DdxnryQ1sHxeklsHx5HkWUmu2szjVv94P0nygyTvSrLdlva/9zTgN4A9qmq/mRpvS6rqrVX1CoAke/bbffv57hdAVf1bVT1qPh6736/vSLLbfDz+XZXkxCR/twXtD0yydvC6wX1D0twxKEsze25V7Qg8BLgGeO889uU9wP8AjgPuD+wDfBp4zkCb64E/n2E9PwX+1xY+9mP77XAQ8GLglVt4f/rQ9zDgqqr66Szvrzk2n9s9yX2A3wZuBP7bfPVD0rbJoCyNqKpuBT4JLJu6LslzknwjyU1J1iQ5ceC2eyb5uyTXJVmf5MIkD+pvOy/JKwbavjLJZUluTnJpkicMP36SvYE/AI6sqn+tqp9X1S1V9Ymq+suBph8Dfi3JAZsZzsnAkUn2msV2+H/AvwH79v3aLcmnkqxLcmWS4wb6fGKST/bb4SbgaOBDwFP62ek3DYx/dZLrk6wYnDnsZ1X/IMl3gO8MXPf7Sb7Tb7M3J3lkkgv65+KsJDv0be+X5DN9/27oL+8xsP7z+vt/uV/X55PsOnD705L8R/8crknysv76eyR5Z5LvJ7kmyfuT3Ku1zZJ8L8kT+8sv6fu/rF9+RZJPD2yvqZnHL/X/ru+31VMG1vfOfixXJjlkmsc8Psknh657T5KT+8svH9jnrkjyuwPtDkx3tOJ1SX4EfGR4ljPJY/pttz7JJUkOG9qmg/v3naeMpPPuJNcmuTHJxUn2bY2h99vAeuAkYKPTldLNNP/5wPJwH5/Qvz5vTnfk5cyp9gNjfG3flx8meV6SQ5N8u98XXz+wrrv12/S76V7TZyW5f3/b1Oz/Uf3+8OMkb+hvOxh4PfCi/nn85ua2f7o3Bp8Fduvb/yTda2yjWekkh/XbfX2/vR8zcNtVSV7Tb9sb+3HfczPbWNI0DMrSiJLcG3gRsHLg6p8CLwV2oZvV/b0kz+tvOwrYGVgKPAB4FfCzxnp/BzixX899gcOA6xpdOAhYW1Uzndd7C/BW4C2bafMD4IP9426RPuA9HfhGkrsB/wR8E9i97+MfJfmvA3c5nO4Nxi7A39JthwuqaseqemOSXwf+Angh3az994Azhh72ecCTGXiTAhwMPBHYH3gtcCrdjONSuhB/ZN/ubsBH6GayH0r3HPz10PpfDLwceCCwA/CafqwPpQst7wWWAI8DLurv8za6Gf3HAXv14z9hms12PnBgf/kZwBXAAQPL5zfu84z+3136bXVBv/xk4HJgV+DtwN8kSeP+pwOHJrlvP5bt6Lbxaf3t1wK/SbfPvRx4dzZ+g/ZguqMWDwOOGVxxkrvTPe+fp9tmfwh8Iskop2Y8ux/bPnT7xIto7+9TjurHcgbw6DTeRLake6P0D8BH+3GcDjx/qNmDgXuy4bn7IPASuv3q6cAJSR7Rtz2Obj88ANgNuAE4ZWh9TwMeRfc6OCHJY6rqc3SvxzP75/Gxfdvm9u+PtBwCXN2337Gqrh4a2z79eP6Ibr88B/infsxTXkj3Gnk48GvAy0bYbJKGGJSlmX06yXrgJrpza98xdUNVnVdV/1lVv6yqi+n+85oKQLfTBeS9quoXVfW1qrqpsf5XAG+vqgurs7qqvtdo9wDghyP2+QPAQ6ebbez9BfDcJL8y4jq/nuQGuoD0Ibrw+SRgSVWdVFW3VdUVdGHjiIH7XVBVn+630SZvFOjC7Yer6utV9XPgT+lmnPcc7GtVXT90/7dV1U1VdQnwLeDzVXVFVd1IF24fD1BV11XVp/rZ95vp3kAMz7Z/pKq+3a//LLrwO9W3f6mq06vq9n5dF/XB9JXA/+z7dTNdGDqCtvMHHvPpdNt+avkA2kF5Ot+rqg9W1S/ojh48BHjQcKN+H/o6XbgD+HXglqpa2d/+z1X13X6fO58u9D59YBW/BN7YH7kYft72B3YE/rJ/3v8V+Awb3pxszu3ATsCjgVTVZVXV3K/7NyrPBE6rqmuALzA0q7wZ+wPbAyf3z93fA8NvMm8H3lJVt9MF8V2B91TVzf1+dQldyAT4XeANVbW2309PBF6QjU9LeVNV/ayqvkn35vGxTGOE7b85LwL+uar+b9/3dwL3Av7LQJuTq+rqqrqe7jX7uMZ6JM3AoCzN7HlVtQtwD+BY4PwkDwZI8uQkX0x3WP9GutnSqcP2HwfOBc5IcnWSt/czccOWAt8doR/X0YWiGfX/kb+5/2vNNlJV6+hmVk8aZZ3AE6rqflX1yKr6s6r6Jd1s42794d/1/RuK17NxcFszw3p3o5tFnurXT+jGuvsM67hm4PLPGss7QnckIMkH0p3+cBPdKQ27ZOMPI/5o4PItU/dl+udmCXBv4GsD4/5cf33L+cDT+/1mO+BM4Kn9m4Gd2TBLPYo7+1pVt/QXd5ym7WlsCK8vZsNsMkkOSbKyP8VgPXAoG/ZdgHX96UYtuwFr+n1gyvfY+Dlr6kP1X9PNxl6T5NSpWe+G/w5cVlVT2+cTwIuneR21+viDqqqB64b3o+v6Nxyw4WhPcz+i29f/YeD5vgz4BRvv69PtR5sYYftvzvBr5pd0Yxvc/iP3RdL0DMrSiPpZ4b+n+8/xaf3VpwErgKVVtTPwfvpg2s9ivamqltHN9Pwm3ekVw9YAjxyhC18A9kiyfMQuf4QuhA0fbh70DroZuyeOuM5ha4Arq2qXgb+dqurQgTY13Z17V9OFEODOczQfQHd6yKjr2JxX0x0Of3JV3ZcNpzQ030AMme65+TFdiPqVgXHvXN2HHTdRVavpwspxwJf6Gegf0Z3S8O9DgfPOu43Qv5mcDRyY7pzs59MH5ST3AD5FNxP5oP6N4DlsvE029/hXA0v7U2+mPJQNz9lP6d5ITHnw4J2r6uSqeiLwK3SnYPzJNI/zUuARSX6U7lzpd9GFyakjJZt7nB8Cuw+dlnJXvrFmDXDI0L5+z6r6wYz3HNqWI2z/LX3NhG5so/RF0hYwKEsjSudw4H50s0nQHUK+vqpuTfeVbC8eaP/MJL/az1zeRHeY9xfD66U7jeE1SZ7YP8ZeSR423KiqvgO8Dzg93QeRdkj3gcEjkhzfaH8H3eHh1003pqpaD/wV3Tm+s/FV4KZ0H/q6V5Ltkuyb5ElbsI7TgJcneVwfIN4KfKWqrppln4btRBdq16f78NUbt+C+nwCeleSFSbZP8oAkj+uD7Qfpzit9IECS3YfOzR52Pv0RiX75vKHlYevoTn94xDS3z6g/anAe3ZumK6tqar/dge4IyTrgjv4UnWdvwaq/QhdSX5vk7kkOBJ7LhnPLLwJ+q5/N34vuQ5wAJHlSfyTm7v06bqXxukj34cVHAvvRnTbwOLpzz09jw+kXF9Gdh33/frb+jwZWcUG/3mP75+7wfl2z9X7gLVOvzSRL+nWO4hpgz4E3FjNt/2uAByTZeZr1nQU8J8lB/XZ8NfBz4D+2aESSZmRQlmb2T0l+Qhd23wIc1Z+/CPD7wElJbqb7MNBZA/d7MN2H2G6iC9bnA5t8l2pVnd2v9zTgZrqve7v/NH05jg2HrdfTnRbwfLpzEFtOZ+bzmt9DO8DPqD9s/Vy6EHMl3Uzrh+hmskddxxfovqruU31fH8n05/rOxv+mO3/zx3QfxPzcFvTt+3SHxF9N97V7F7HhvNPXAauBlf0pHf9CN3M9nfPpQvuXplkefuxb6PaLL/eH+/cftd9DTgOexcBpF/2M9nF0++sNdG/wVoy6wqq6je5Dp4fQbdf3AS+t7htRAN4N3EYX+D5G94Zjyn3p3mTcQHf6wHV0M6vDjgL+sbrPAPxo6o9uf/3N/k3Px+nOBb6K7hzfM4f6+Ft0IX093Yf0PkMXKGfjPXTb6PP9630l3QcrR3F2/+91Sb4+0/bvt+PpwBX9c7/R90dX1eX9eN5Lt/2fS/c1lrfNcmySppGNT9+SJGlxSvIV4P1V9ZH57oukhcEZZUnSopTkgCQP7k+9OIruGyxGPqIgSf7KlSRpsXoU3ekNO9KdpvSC6b6KTpJaPPVCkiRJavDUC0mSJKnBoCxJkiQ1GJQlSZKkBoOyJEmS1GBQliRJkhpmDMpJPpzk2iTfmub2JDk5yeokFyd5wvi7KUmaJGu9JG1qlBnljwIHb+b2Q4C9+79jgP9z17slSZpjH8VaL0kbmTEoV9WXgOs30+Rw4G+rsxLYJclDxtVBSdLkWeslaVPjOEd5d2DNwPLa/jpJ0uJhrZe0zRnHT1incV3z5/6SHEN3yI773Oc+T3z0ox89hoeXpNn72te+9uOqWjLf/VgArPWSFqzZ1vpxBOW1wNKB5T2Aq1sNq+pU4FSA5cuX16pVq8bw8JI0e0m+N999WCCs9ZIWrNnW+nGcerECeGn/iej9gRur6odjWK8kaethrZe0zZlxRjnJ6cCBwK5J1gJvBO4OUFXvB84BDgVWA7cAL59UZyVJk2Gtl6RNzRiUq+rIGW4v4A/G1iNJ0pyz1kvSpvxlPkmSJKnBoCxJkiQ1GJQlSZKkBoOyJEmS1GBQliRJkhoMypIkSVKDQVmSJElqMChLkiRJDQZlSZIkqcGgLEmSJDUYlCVJkqQGg7IkSZLUYFCWJEmSGgzKkiRJUoNBWZIkSWowKEuSJEkNBmVJkiSpwaAsSZIkNRiUJUmSpAaDsiRJktRgUJYkSZIaDMqSJElSg0FZkiRJajAoS5IkSQ0GZUmSJKnBoCxJkiQ1GJQlSZKkBoOyJEmS1GBQliRJkhoMypIkSVKDQVmSJElqMChLkiRJDQZlSZIkqcGgLEmSJDUYlCVJkqQGg7IkSZLUMFJQTnJwksuTrE5yfOP2hyb5YpJvJLk4yaHj76okaVKs85K0qRmDcpLtgFOAQ4BlwJFJlg01+zPgrKp6PHAE8L5xd1SSNBnWeUlqG2VGeT9gdVVdUVW3AWcAhw+1KeC+/eWdgavH10VJ0oRZ5yWpYZSgvDuwZmB5bX/doBOBlyRZC5wD/GFrRUmOSbIqyap169bNoruSpAkYW50Ha72kxWOUoJzGdTW0fCTw0araAzgU+HiSTdZdVadW1fKqWr5kyZIt760kaRLGVufBWi9p8RglKK8Flg4s78Gmh9yOBs4CqKoLgHsCu46jg5KkibPOS1LDKEH5QmDvJA9PsgPdhzhWDLX5PnAQQJLH0BVQj7dJ0sJgnZekhhmDclXdARwLnAtcRvep50uSnJTksL7Zq4FXJvkmcDrwsqoaPmwnSdoKWeclqW37URpV1Tl0H94YvO6EgcuXAk8db9ckSXPFOi9Jm/KX+SRJkqQGg7IkSZLUYFCWJEmSGgzKkiRJUoNBWZIkSWowKEuSJEkNBmVJkiSpwaAsSZIkNRiUJUmSpAaDsiRJktRgUJYkSZIaDMqSJElSg0FZkiRJajAoS5IkSQ0GZUmSJKnBoCxJkiQ1GJQlSZKkBoOyJEmS1GBQliRJkhoMypIkSVKDQVmSJElqMChLkiRJDQZlSZIkqcGgLEmSJDUYlCVJkqQGg7IkSZLUYFCWJEmSGgzKkiRJUoNBWZIkSWowKEuSJEkNBmVJkiSpwaAsSZIkNRiUJUmSpAaDsiRJktRgUJYkSZIaDMqSJElSw0hBOcnBSS5PsjrJ8dO0eWGSS5NckuS08XZTkjRJ1nlJ2tT2MzVIsh1wCvAbwFrgwiQrqurSgTZ7A38KPLWqbkjywEl1WJI0XtZ5SWobZUZ5P2B1VV1RVbcBZwCHD7V5JXBKVd0AUFXXjrebkqQJss5LUsMoQXl3YM3A8tr+ukH7APsk+XKSlUkObq0oyTFJViVZtW7dutn1WJI0bmOr82Ctl7R4jBKU07iuhpa3B/YGDgSOBD6UZJdN7lR1alUtr6rlS5Ys2dK+SpImY2x1Hqz1khaPUYLyWmDpwPIewNWNNv9YVbdX1ZXA5XQFVZK09bPOS1LDKEH5QmDvJA9PsgNwBLBiqM2ngWcCJNmV7hDdFePsqCRpYqzzktQwY1CuqjuAY4FzgcuAs6rqkiQnJTmsb3YucF2SS4EvAn9SVddNqtOSpPGxzktSW6qGT0ObG8uXL69Vq1bNy2NL0pQkX6uq5fPdj8XKWi9pazDbWu8v80mSJEkNBmVJkiSpwaAsSZIkNRiUJUmSpAaDsiRJktRgUJYkSZIaDMqSJElSg0FZkiRJajAoS5IkSQ0GZUmSJKnBoCxJkiQ1GJQlSZKkBoOyJEmS1GBQliRJkhoMypIkSVKDQVmSJElqMChLkiRJDQZlSZIkqcGgLEmSJDUYlCVJkqQGg7IkSZLUYFCWJEmSGgzKkiRJUoNBWZIkSWowKEuSJEkNBmVJkiSpwaAsSZIkNRiUJUmSpAaDsiRJktRgUJYkSZIaDMqSJElSg0FZkiRJajAoS5IkSQ0GZUmSJKnBoCxJkiQ1jBSUkxyc5PIkq5Mcv5l2L0hSSZaPr4uSpEmzzkvSpmYMykm2A04BDgGWAUcmWdZotxNwHPCVcXdSkjQ51nlJahtlRnk/YHVVXVFVtwFnAIc32r0ZeDtw6xj7J0maPOu8JDWMEpR3B9YMLK/tr7tTkscDS6vqM2PsmyRpbljnJalhlKCcxnV1543J3YB3A6+ecUXJMUlWJVm1bt260XspSZqksdX5vr21XtKiMEpQXgssHVjeA7h6YHknYF/gvCRXAfsDK1of9KiqU6tqeVUtX7Jkyex7LUkap7HVebDWS1o8RgnKFwJ7J3l4kh2AI4AVUzdW1Y1VtWtV7VlVewIrgcOqatVEeixJGjfrvCQ1zBiUq+oO4FjgXOAy4KyquiTJSUkOm3QHJUmTZZ2XpLbtR2lUVecA5wxdd8I0bQ+8692SJM0l67wkbcpf5pMkSZIaDMqSJElSg0FZkiRJajAoS5IkSQ0GZUmSJKnBoCxJkiQ1GJQlSZKkBoOyJEmS1GBQliRJkhoMypIkSVKDQVmSJElqMChLkiRJDQZlSZIkqcGgLEmSJDUYlCVJkqQGg7IkSZLUYFCWJEmSGgzKkiRJUoNBWZIkSWowKEuSJEkNBmVJkiSpwaAsSZIkNRiUJUmSpAaDsiRJktRgUJYkSZIaDMqSJElSg0FZkiRJajAoS5IkSQ0GZUmSJKnBoCxJkiQ1GJQlSZKkBoOyJEmS1GBQliRJkhoMypIkSVKDQVmSJElqMChLkiRJDSMF5SQHJ7k8yeokxzdu/+Mklya5OMkXkjxs/F2VJE2KdV6SNjVjUE6yHXAKcAiwDDgyybKhZt8AllfVrwGfBN4+7o5KkibDOi9JbaPMKO8HrK6qK6rqNuAM4PDBBlX1xaq6pV9cCewx3m5KkibIOi9JDaME5d2BNQPLa/vrpnM08NnWDUmOSbIqyap169aN3ktJ0iSNrc6DtV7S4jFKUE7jumo2TF4CLAfe0bq9qk6tquVVtXzJkiWj91KSNEljq/NgrZe0eGw/Qpu1wNKB5T2Aq4cbJXkW8AbggKr6+Xi6J0maA9Z5SWoYZUb5QmDvJA9PsgNwBLBisEGSxwMfAA6rqmvH301J0gRZ5yWpYcagXFV3AMcC5wKXAWdV1SVJTkpyWN/sHcCOwNlJLkqyYprVSZK2MtZ5SWob5dQLquoc4Jyh604YuPysMfdLkjSHrPOStCl/mU+SJElqMChLkiRJDQZlSZIkqcGgLEmSJDUYlCVJkqQGg7IkSZLUYFCWJEmSGgzKkiRJUoNBWZIkSWowKEuSJEkNBmVJkiSpwaAsSZIkNRiUJUmSpAaDsiRJktRgUJYkSZIaDMqSJElSg0FZkiRJajAoS5IkSQ0GZUmSJKnBoCxJkiQ1GJQlSZKkBoOyJEmS1GBQliRJkhoMypIkSVKDQVmSJElqMChLkiRJDQZlSZIkqcGgLEmSJDUYlCVJkqQGg7IkSZLUYFCWJEmSGgzKkiRJUoNBWZIkSWowKEuSJEkNBmVJkiSpYaSgnOTgJJcnWZ3k+Mbt90hyZn/7V5LsOe6OSpImxzovSZuaMSgn2Q44BTgEWAYcmWTZULOjgRuqai/g3cDbxt1RSdJkWOclqW2UGeX9gNVVdUVV3QacARw+1OZw4GP95U8CByXJ+LopSZog67wkNYwSlHcH1gwsr+2va7apqjuAG4EHjKODkqSJs85LUsP2I7RpzRjULNqQ5BjgmH7x50m+NcLjL3S7Aj+e707MkW1lrI5zcXnUfHdgKzC2Og/W+kXOcS4u28o4YZa1fpSgvBZYOrC8B3D1NG3WJtke2Bm4fnhFVXUqcCpAklVVtXw2nV5ItpVxwrYzVse5uCRZNd992AqMrc6DtX4xc5yLy7YyTph9rR/l1IsLgb2TPDzJDsARwIqhNiuAo/rLLwD+taqaMw2SpK2OdV6SGmacUa6qO5IcC5wLbAd8uKouSXISsKqqVgB/A3w8yWq6GYYjJtlpSdL4WOclqW2UUy+oqnOAc4auO2Hg8q3A72zhY5+6he0Xqm1lnLDtjNVxLi7byjg3a0J1Hrad7es4FxfHufjMaqzxyJkkSZK0KX/CWpIkSWqYeFDeVn4WdYRx/nGSS5NcnOQLSR42H/28q2Ya50C7FySpJAvy07SjjDPJC/vn9JIkp811H8dlhH33oUm+mOQb/f576Hz0865I8uEk1073NWXpnNxvg4uTPGGu+7iQWefvvH1R1Hmw1g+1WfC1fluo8zChWl9VE/uj+1DId4FHADsA3wSWDbX5feD9/eUjgDMn2ad5HOczgXv3l39vsY6zb7cT8CVgJbB8vvs9oedzb+AbwP365QfOd78nONZTgd/rLy8Drprvfs9inM8AngB8a5rbDwU+S/ddwfsDX5nvPi+UP+v8Rm0WfJ0fdax9O2v9AvjbVup83/ex1/pJzyhvKz+LOuM4q+qLVXVLv7iS7ntKF5pRnk+ANwNvB26dy86N0SjjfCVwSlXdAFBV185xH8dllLEWcN/+8s5s+v26W72q+hLTfOdv73Dgb6uzEtglyUPmpncLnnW+t0jqPFjrBy2GWr9N1HmYTK2fdFDeVn4WdZRxDjqa7h3NQjPjOJM8HlhaVZ+Zy46N2SjP5z7APkm+nGRlkoPnrHfjNcpYTwRekmQt3bci/OHcdG1ObelrWBtY59sWap0Ha/2gxVDrrfMbbHGtH+nr4e6Csf4s6lZsS37a9SXAcuCAifZoMjY7ziR3A94NvGyuOjQhozyf29MdkjuQbtbo35LsW1XrJ9y3cRtlrEcCH62qv0ryFLrv0t23qn45+e7NmcVQh+aLdX644cKu82CtH7QYar11foMtrkWTnlHekp9FJTP8LOpWbJRxkuRZwBuAw6rq53PUt3GaaZw7AfsC5yW5iu78nxUL8EMeo+63/1hVt1fVlcDldMV0oRllrEcDZwFU1QXAPYFd56R3c2ek17CarPMDFkGdB2v9cJuFXuut8xtsca2fdFDeVn4WdcZx9oepPkBXPBfiOU4wwzir6saq2rWq9qyqPenO0Tusqmb1++rzaJT99tN0H9whya50h+eumNNejscoY/0+cBBAksfQFdB1c9rLyVsBvLT/RPT+wI1V9cP57tQCYZ3vLZI6D9b6QYuh1lvnN9jyWj8Hn0A8FPg23Scu39BfdxLdiwq6J+NsYDXwVeARk+7TPI3zX4BrgIv6vxXz3edJjHOo7XkswE9Cj/h8BngXcCnwn8AR893nCY51GfBluk9KXwQ8e777PIsxng78ELidbkbhaOBVwKsGns9T+m3wnwt1v92K9yHr/AL7s9Yvrlq/LdT5fhxjr/X+Mp8kSZLU4C/zSZIkSQ0GZUmSJKnBoCxJkiQ1GJQlSZKkBoOyJEmS1GBQliRJkhoMypIkSVKDQVmSJElq+P/wW0deZULKEwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "t = f.suptitle('Basic CNN Performance with various Augmentation', fontsize=12)\n",
    "f.subplots_adjust(top=0.90, wspace=0.3)\n",
    "\n",
    "epoch_list = list(range(1,125))\n",
    "ax1.plot(epoch_list, history.history['acc'], label='Train Accuracy')\n",
    "ax1.plot(epoch_list, history.history['val_acc'], label='Validation Accuracy')\n",
    "ax1.set_xticks(np.arange(0, 124, 125))\n",
    "ax1.set_ylabel('Accuracy Value')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Accuracy')\n",
    "l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n",
    "ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_xticks(np.arange(0, 124, 125))\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Loss')\n",
    "l2 = ax2.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Untitled2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
